{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "# Display Formatting\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset extraction and organization\n",
    "### Load and combine all datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure data folder exists\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Load datasets\n",
    "prices_df = pd.read_csv(\"data/prices-split-adjusted.csv\")\n",
    "securities_df = pd.read_csv(\"data/securities.csv\")\n",
    "\n",
    "# Display first few rows (optional in scripts, great for notebooks)\n",
    "prices_df.head(), securities_df.head()\n",
    "\n",
    "# Filter IT sector companies\n",
    "it_companies = securities_df[securities_df['GICS Sector'] == 'Information Technology']\n",
    "\n",
    "# Select 10 companies (including AAPL)\n",
    "selected_companies = ['AAPL', 'MSFT', 'ORCL', 'IBM', 'INTC', 'CSCO', 'HPQ', 'ADBE', 'NVDA', 'TXN']\n",
    "print(f\"Selected Companies: {', '.join(selected_companies)}\")\n",
    "\n",
    "# Filter prices\n",
    "filtered_prices = prices_df[prices_df['symbol'].isin(selected_companies)].copy()\n",
    "filtered_prices['date'] = pd.to_datetime(filtered_prices['date'])\n",
    "filtered_prices = filtered_prices.sort_values(by=['symbol', 'date'])\n",
    "filtered_prices = filtered_prices.dropna().drop_duplicates()\n",
    "filtered_prices = filtered_prices[['date', 'symbol', 'open', 'close', 'volume']]\n",
    "\n",
    "\n",
    "# Model 1: Only AAPL  model1_data = df[df['symbol'] == 'AAPL']\n",
    "\n",
    "# model_1_data = filtered_prices[filtered_prices['symbol'] == 'AAPL']\n",
    "# model_1_data.to_csv(\"data/model1_data_AAPL.csv\", index=False)\n",
    "\n",
    "# Model 2: All 10 companies\n",
    "model_2_data = filtered_prices\n",
    "model_2_data.to_csv(\"data/model2_data_all_companies.csv\", index=False)\n",
    "\n",
    "# Confirmation\n",
    "print(\"Saved model datasets to 'data/' folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "df = pd.read_csv('data/model2_data_all_companies.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "### Visualizing and analyzing sentiment distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the filtered and cleaned data\n",
    "df = pd.read_csv('data/model2_data_all_companies.csv')\n",
    "\n",
    "# Converting the date column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "#Sorting data by company and date( we did it in preprocessing, bbut just to be sure since it is a crucial step)\n",
    "df= df.sort_values(by=['symbol', 'date'])\n",
    "\n",
    "# Creating a daily return feature\n",
    "df['daily_change'] = (df['close'] - df['open']) / df['open'] # measuring the percentage change in the stock price daily for each index\n",
    "\n",
    "# creating the volum change feature relative to previous day\n",
    "df['volume_change'] = df.groupby('symbol')['volume'].diff() # Measuring how the volume changed from the previous day.\n",
    "\n",
    "# Creating a rolling average of the closing price over the past 10 days\n",
    "df['rolling_close_mean'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(window=10).mean()) # Each row gets a new value, the 10-day average at that row.\n",
    "\n",
    "# creaing a rolling average of the volume over the past 10 days\n",
    "df['rolling_volume_mean'] = df.groupby('symbol')['volume'].transform(lambda x: x.rolling(window=10).mean()) # Each row gets a new value, the 10-day average at that row.\n",
    "\n",
    "# creating the target variable binary to see the next day's price is higher than today's price or not\n",
    "df['next_close'] = df.groupby('symbol')['close'].shift(-1) \n",
    "df['target'] = (df['next_close'] > df['close']).astype(int) # if row['next_close'] > row['close']: return 1 else: return 0\n",
    "\n",
    "# Dropping rows with missing values caused by rolling or shifting operations\n",
    "df = df.dropna() # first 9 rows becuase of roling windows and the last because of shifting operations\n",
    "\n",
    "# Checking the final result\n",
    "display(df[['date', 'symbol', 'open', 'close', 'daily_change', 'volume_change', 'rolling_close_mean', 'rolling_volume_mean', 'target']].head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis After feature engineering\n",
    "### Visualizing and analyzing sentiment distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Comparison of Closing Prices and Trading Volumes (AAPL vs. IT Sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter Model 1 (AAPL only) and Model 2 (all IT companies)\n",
    "model_1_data = df[df['symbol'] == 'AAPL']\n",
    "model_2_data = df  # already contains all 10 companies\n",
    "\n",
    "# ---- Closing Price Distribution ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.histplot(model_1_data['close'], bins=50, label='AAPL', kde=True, color='blue')\n",
    "sns.histplot(model_2_data['close'], bins=50, label='All IT Companies', kde=True, color='red', alpha=0.5)\n",
    "plt.xlabel(\"Closing Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Closing Prices (AAPL vs IT Sector)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---- Trading Volume Distribution ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.histplot(model_1_data['volume'], bins=50, label='AAPL', kde=True, color='blue')\n",
    "sns.histplot(model_2_data['volume'], bins=50, label='All IT Companies', kde=True, color='red', alpha=0.5)\n",
    "plt.xlabel(\"Trading Volume\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Trading Volume (AAPL vs IT Sector)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Price Trends Over Time (AAPL vs IT Sector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# AAPL only\n",
    "sns.lineplot(data=model_1_data, x='date', y='close', label='AAPL', color='blue')\n",
    "\n",
    "# All other IT companies (excluding AAPL to avoid duplicate line)\n",
    "other_companies = model_2_data[model_2_data['symbol'] != 'AAPL']\n",
    "sns.lineplot(data=other_companies, x='date', y='close', hue='symbol', alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Closing Price\")\n",
    "plt.title(\"Stock Price Trends Over Time (AAPL vs IT Sector)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading Volume Trends Over Time (AAPL vs IT Sector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# AAPL volume\n",
    "sns.lineplot(data=model_1_data, x='date', y='volume', label='AAPL', color='blue')\n",
    "\n",
    "# Volume for other IT companies\n",
    "sns.lineplot(data=other_companies, x='date', y='volume', hue='symbol', alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Trading Volume\")\n",
    "plt.title(\"Trading Volume Trends Over Time (AAPL vs IT Sector)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of Pairwise Correlations Between Stock Closing Prices (IT Sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data: each column is one company, rows are dates, values are closing prices\n",
    "pivot_data = df.pivot(index='date', columns='symbol', values='close')\n",
    "\n",
    "# Compute correlation matrix between the companies\n",
    "correlation_matrix = pivot_data.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Matrix of Stock Closing Prices (IT Sector)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable Distribution: Price Increase vs. No Increase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(data=df, x='target')\n",
    "plt.title('Target Distribution (0 = price down or same, 1 = price up)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix of Engineered Features and Target Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[['daily_change', 'volume_change', 'rolling_close_mean', 'rolling_volume_mean', 'target']].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AAPL Closing Price vs. 10-Day Rolling Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# We don't use above library Because train_test_split randomly shuffles the data, which is not appropriate for time series like stock prices.\n",
    "\n",
    "\n",
    "# Filteting Model 1 ( AAPL only ) and Model 2 ( AAPL + others )\n",
    "# using copy method to avoid unintended changes\n",
    "model_1_data = df[df['symbol'] == 'AAPL'].copy()\n",
    "model_2_data = df.copy()\n",
    "\n",
    "#Dropping unnecessary columns \n",
    "drop_cols = ['symbol', 'next_close', 'date'] # these colums are not featyres for our models\n",
    "model_1_data = model_1_data.drop(columns=drop_cols)\n",
    "model_2_data = model_2_data.drop(columns=drop_cols)\n",
    "\n",
    "# Chronologically splitting data into train and test sets( 80% train, 20% test )\n",
    "# We don't use sklearn's train_test_split because we want to keep the chronological order because it shuffles the data\n",
    "\n",
    "#Model 1 (APPL Only)\n",
    "model_1_split_index = int(len(model_1_data) * 0.8) # 80% index for train set\n",
    "model_1_train = model_1_data.iloc[:model_1_split_index] # 80% of rows for train set\n",
    "model_1_test = model_1_data.iloc[model_1_split_index:] # 20% of rows for test set\n",
    "\n",
    "#Model 2 (APPL + Others)\n",
    "model_2_split_index = int(len(model_2_data) * 0.8) # 80% index for train set\n",
    "model_2_train = model_2_data.iloc[:model_2_split_index] # 80% of rows for train set\n",
    "model_2_test = model_2_data.iloc[model_2_split_index:] # 20% of rows for test set\n",
    "\n",
    "# separating features (x) and target (y)\n",
    "# The model will learn from features (x) to predict the binary target (y)\n",
    "\n",
    "# Model 1\n",
    "x__train_1 = model_1_train.drop(columns='target') # Features input for training (daily_change, volume_change, rolling_close_mean, rolling_volume_mean are independent variables)\n",
    "y__train_1 = model_1_train['target'] # utput for training (dependent variable (0 or 1))\n",
    "x__test_1 = model_1_test.drop(columns='target')\n",
    "y__test_1 = model_1_test['target']\n",
    "\n",
    "# Model 2\n",
    "x__train_2 = model_2_train.drop(columns='target') # Features input for training (daily_change, volume_change, rolling_close_mean, rolling_volume_mean are independent variables)\n",
    "y__train_2 = model_2_train['target']\n",
    "x__test_2 = model_2_test.drop(columns='target')\n",
    "y__test_2 = model_2_test['target']\n",
    "\n",
    "\n",
    "# The shapes of eac dataset\n",
    "print('Model 1 ( AAPL only ):')\n",
    "print(f'Training set: {x__train_1.shape}, Testing set: {x__test_1.shape}')\n",
    "print('Model 2 ( AAPL + Others ):')\n",
    "print(f'Training set: {x__train_2.shape}, Testing set: {x__test_2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model to test everything works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # --------------------------------\n",
    "# # MODEL 1: Logistic Regression on AAPL Only\n",
    "# # --------------------------------\n",
    "\n",
    "# # Initialize and train the model\n",
    "# model_1_lr = LogisticRegression(max_iter=1000)\n",
    "# model_1_lr.fit(x__train_1, y__train_1)\n",
    "\n",
    "# # Predict on test set\n",
    "# y_pred_1 = model_1_lr.predict(x__test_1)\n",
    "# y_proba_1 = model_1_lr.predict_proba(x__test_1)[:, 1]  # For ROC AUC\n",
    "\n",
    "# # Evaluation metrics\n",
    "# print(\" Model 1 (AAPL Only) Evaluation\")\n",
    "# print(\"Accuracy:\", accuracy_score(y__test_1, y_pred_1))\n",
    "# print(\"Classification Report:\\n\", classification_report(y__test_1, y_pred_1))\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y__test_1, y_pred_1))\n",
    "# print(\"ROC AUC Score:\", roc_auc_score(y__test_1, y_proba_1))\n",
    "\n",
    "\n",
    "# # --------------------------------\n",
    "# # MODEL 2: Logistic Regression on AAPL + Other IT Companies\n",
    "# # --------------------------------\n",
    "\n",
    "# # Initialize and train the model\n",
    "# model_2_lr = LogisticRegression(max_iter=1000)\n",
    "# model_2_lr.fit(x__train_2, y__train_2)\n",
    "\n",
    "# # Predict on test set\n",
    "# y_pred_2 = model_2_lr.predict(x__test_2)\n",
    "# y_proba_2 = model_2_lr.predict_proba(x__test_2)[:, 1]\n",
    "\n",
    "# # Evaluation metrics\n",
    "# print(\"\\n Model 2 (All IT Companies) Evaluation\")\n",
    "# print(\"Accuracy:\", accuracy_score(y__test_2, y_pred_2))\n",
    "# print(\"Classification Report:\\n\", classification_report(y__test_2, y_pred_2))\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y__test_2, y_pred_2))\n",
    "# print(\"ROC AUC Score:\", roc_auc_score(y__test_2, y_proba_2))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
