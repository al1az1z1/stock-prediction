{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System / OS Handling (Standard Library)\n",
    "import os\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    roc_curve, \n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Display Formatting\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset extraction and organization\n",
    "### Load and combine all datasets\n",
    "### Data Cleaning and Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data folder exists\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Load datasets\n",
    "prices_df = pd.read_csv(\"data/prices-split-adjusted.csv\")\n",
    "securities_df = pd.read_csv(\"data/securities.csv\")\n",
    "\n",
    "# Display first few rows (optional in scripts, great for notebooks)\n",
    "prices_df.head(), securities_df.head()\n",
    "\n",
    "# Filter IT sector companies\n",
    "it_companies = securities_df[securities_df['GICS Sector'] == 'Information Technology']\n",
    "\n",
    "# Select 10 companies (including AAPL)\n",
    "selected_companies = ['AAPL', 'MSFT', 'ORCL', 'IBM', 'INTC', 'CSCO', 'HPQ', 'ADBE', 'NVDA', 'TXN']\n",
    "print(f\"Selected Companies: {', '.join(selected_companies)}\")\n",
    "\n",
    "# Filter prices\n",
    "# Keeping only rows for the selected IT sector companies from the full price dataset.\n",
    "filtered_prices = prices_df[prices_df['symbol'].isin(selected_companies)].copy()\n",
    "\n",
    "# Converting the 'date' column from string format to datetime objects.\n",
    "filtered_prices['date'] = pd.to_datetime(filtered_prices['date'])\n",
    "\n",
    "# Sorting the data by 'symbol' and 'date' to ensure proper chronological order within each stock.\n",
    "filtered_prices = filtered_prices.sort_values(by=['symbol', 'date'])\n",
    "\n",
    "# Checking how many fully duplicated rows are in the dataset\n",
    "print(\"Number of duplicated rows:\", filtered_prices.duplicated().sum())\n",
    "\n",
    "# Dropping rows with missing values and duplicates.\n",
    "filtered_prices = filtered_prices.dropna().drop_duplicates()\n",
    "\n",
    "# Keeping only the necessary columns\n",
    "filtered_prices = filtered_prices[['date', 'symbol', 'open', 'close', 'volume']]\n",
    "\n",
    "# Assigning the cleaned and filtered dataset to a new variable 'df' for convenience.\n",
    "df= filtered_prices\n",
    "\n",
    "# Displaying the first few rows of the dataset to visually inspect the structure and confirm the data looks correct.\n",
    "print(\"=====================================================\")\n",
    "print(\"Preview of cleaned dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "\n",
    "# Generating a statistical summary of the dataset's numerical columns.\n",
    "print(\"=====================================================\")\n",
    "print(\"Statistical summary of the dataset:\")\n",
    "df.describe()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the filtered and cleaned data\n",
    "df = pd.read_csv('data/model2_data_all_companies.csv')\n",
    "\n",
    "# Converting the date column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "#Sorting data by company and date( we did it in preprocessing, bbut just to be sure since it is a crucial step)\n",
    "df= df.sort_values(by=['symbol', 'date'])\n",
    "\n",
    "# Creating a daily return feature\n",
    "df['daily_change'] = (df['close'] - df['open']) / df['open'] # measuring the percentage change in the stock price daily for each index\n",
    "\n",
    "# creating the volum change feature relative to previous day\n",
    "df['volume_change'] = df.groupby('symbol')['volume'].diff() # Measuring how the volume changed from the previous day.\n",
    "\n",
    "# Creating a rolling average of the closing price over the past 10 days\n",
    "df['rolling_close_mean'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(window=10).mean()) # Each row gets a new value, the 10-day average at that row.\n",
    "\n",
    "# creaing a rolling average of the volume over the past 10 days\n",
    "df['rolling_volume_mean'] = df.groupby('symbol')['volume'].transform(lambda x: x.rolling(window=10).mean()) # Each row gets a new value, the 10-day average at that row.\n",
    "\n",
    "# creating the target variable binary to see the next day's price is higher than today's price or not\n",
    "df['next_close'] = df.groupby('symbol')['close'].shift(-1) \n",
    "df['target'] = (df['next_close'] > df['close']).astype(int) # if row['next_close'] > row['close']: return 1 else: return 0\n",
    "\n",
    "# Dropping rows with missing values caused by rolling or shifting operations\n",
    "df = df.dropna() # first 9 rows becuase of roling windows and the last because of shifting operations\n",
    "\n",
    "# Checking the final result\n",
    "display(df[['date', 'symbol', 'open', 'close', 'daily_change', 'volume_change', 'rolling_close_mean', 'rolling_volume_mean', 'target']].head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis After feature engineering\n",
    "### Visualizing and analyzing sentiment distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Comparison of Closing Prices and Trading Volumes (AAPL vs. IT Sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Filter Model 1 (AAPL only) and Model 2 (all IT companies)\n",
    "model_1_data = df[df['symbol'] == 'AAPL']\n",
    "model_2_data = df  # already contains all 10 companies\n",
    "\n",
    "# ---- Closing Price Distribution ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.histplot(model_1_data['close'], bins=50, label='AAPL', kde=True, color='blue')\n",
    "sns.histplot(model_2_data['close'], bins=50, label='All IT Companies', kde=True, color='red', alpha=0.5)\n",
    "plt.xlabel(\"Closing Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Closing Prices (AAPL vs IT Sector)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---- Trading Volume Distribution ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.histplot(model_1_data['volume'], bins=50, label='AAPL', kde=True, color='blue')\n",
    "sns.histplot(model_2_data['volume'], bins=50, label='All IT Companies', kde=True, color='red', alpha=0.5)\n",
    "plt.xlabel(\"Trading Volume\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Trading Volume (AAPL vs IT Sector)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Price Trends Over Time (AAPL vs IT Sector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# AAPL only\n",
    "sns.lineplot(data=model_1_data, x='date', y='close', label='AAPL', color='blue')\n",
    "\n",
    "# All other IT companies (excluding AAPL to avoid duplicate line)\n",
    "other_companies = model_2_data[model_2_data['symbol'] != 'AAPL']\n",
    "sns.lineplot(data=other_companies, x='date', y='close', hue='symbol', alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Closing Price\")\n",
    "plt.title(\"Stock Price Trends Over Time (AAPL vs IT Sector)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading Volume Trends Over Time (AAPL vs IT Sector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# AAPL volume\n",
    "sns.lineplot(data=model_1_data, x='date', y='volume', label='AAPL', color='blue')\n",
    "\n",
    "# Volume for other IT companies\n",
    "sns.lineplot(data=other_companies, x='date', y='volume', hue='symbol', alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Trading Volume\")\n",
    "plt.title(\"Trading Volume Trends Over Time (AAPL vs IT Sector)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of Pairwise Correlations Between Stock Closing Prices (IT Sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data: each column is one company, rows are dates, values are closing prices\n",
    "pivot_data = df.pivot(index='date', columns='symbol', values='close')\n",
    "\n",
    "# Compute correlation matrix between the companies\n",
    "correlation_matrix = pivot_data.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Matrix of Stock Closing Prices (IT Sector)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable Distribution: Price Increase vs. No Increase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix of Engineered Features and Target Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[['daily_change', 'volume_change', 'rolling_close_mean', 'rolling_volume_mean', 'target']].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AAPL Closing Price vs. 10-Day Rolling Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# We don't use above library Because:\n",
    "    # this is time series data (stock prices), maintaining chronological order is critical to avoid data leakage.\n",
    "    # Instead, we manually split the data using iloc to preserve the time-based structure.\n",
    "\n",
    "\n",
    "# Filteting Model 1 ( AAPL only ) and Model 2 ( AAPL + others )\n",
    "# using copy method to avoid unintended changes\n",
    "model_1_data = df[df['symbol'] == 'AAPL'].copy()\n",
    "model_2_data = df.copy()\n",
    "\n",
    "#Dropping unnecessary columns \n",
    "drop_cols = ['symbol', 'next_close', 'date', 'open', 'close', 'volume'] # these colums are not featyres for our models\n",
    "model_1_data = model_1_data.drop(columns=drop_cols)\n",
    "model_2_data = model_2_data.drop(columns=drop_cols)\n",
    "\n",
    "# Chronologically splitting data into train and test sets( 80% train, 20% test )\n",
    "# We don't use sklearn's train_test_split because we want to keep the chronological order because it shuffles the data\n",
    "\n",
    "#Model 1 (APPL Only)\n",
    "model_1_split_index = int(len(model_1_data) * 0.8) # 80% index for train set\n",
    "model_1_train = model_1_data.iloc[:model_1_split_index] # 80% of rows for train set\n",
    "model_1_test = model_1_data.iloc[model_1_split_index:] # 20% of rows for test set\n",
    "\n",
    "#Model 2 (APPL + Others)\n",
    "model_2_split_index = int(len(model_2_data) * 0.8) # 80% index for train set\n",
    "model_2_train = model_2_data.iloc[:model_2_split_index] # 80% of rows for train set\n",
    "model_2_test = model_2_data.iloc[model_2_split_index:] # 20% of rows for test set\n",
    "\n",
    "# separating features (x) and target (y)\n",
    "# The model will learn from features (x) to predict the binary target (y)\n",
    "\n",
    "# Model 1\n",
    "x__train_1 = model_1_train.drop(columns='target') # Features input for training (daily_change, volume_change, rolling_close_mean, rolling_volume_mean are independent variables)\n",
    "y__train_1 = model_1_train['target'] # output for training (dependent variable (0 or 1))\n",
    "x__test_1 = model_1_test.drop(columns='target')\n",
    "y__test_1 = model_1_test['target']\n",
    "\n",
    "# Model 2\n",
    "x__train_2 = model_2_train.drop(columns='target') # Features input for training (daily_change, volume_change, rolling_close_mean, rolling_volume_mean are independent variables)\n",
    "y__train_2 = model_2_train['target']\n",
    "x__test_2 = model_2_test.drop(columns='target')\n",
    "y__test_2 = model_2_test['target']\n",
    "\n",
    "# The shapes of eac dataset\n",
    "print('Model 1 ( AAPL only ):')\n",
    "print(f'Training set: {x__train_1.shape}, Testing set: {x__test_1.shape}')\n",
    "print('Model 2 ( AAPL + Others ):')\n",
    "print(f'Training set: {x__train_2.shape}, Testing set: {x__test_2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: AAPL only\n",
    "scaler1 = StandardScaler()\n",
    "x_train_1_scaled = scaler1.fit_transform(x__train_1)\n",
    "x_test_1_scaled = scaler1.transform(x__test_1)\n",
    "\n",
    "clf1 = LogisticRegression(max_iter=1000)\n",
    "clf1.fit(x_train_1_scaled, y__train_1)\n",
    "y_prediction_1 = clf1.predict(x_test_1_scaled)\n",
    "\n",
    "print(accuracy_score(y__test_1, y_prediction_1))\n",
    "print(classification_report(y__test_1, y_prediction_1))\n",
    "\n",
    "\n",
    "# Model 2: 10 IT sector companies\n",
    "scaler2 = StandardScaler()\n",
    "x_train_2_scaled = scaler2.fit_transform(x__train_2)\n",
    "x_test_2_scaled = scaler2.transform(x__test_2)\n",
    "\n",
    "clf2 = LogisticRegression(max_iter=1000)\n",
    "clf2.fit(x_train_2_scaled, y__train_2)\n",
    "y_prediction_2 = clf2.predict(x_test_2_scaled)\n",
    "\n",
    "print(accuracy_score(y__test_2, y_prediction_2))\n",
    "print(classification_report(y__test_2, y_prediction_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Evaluate both models\n",
    "evaluate_model(y__test_1, y_prediction_1, \"Model 1 (AAPL Only)\")\n",
    "evaluate_model(y__test_2, y_prediction_2, \"Model 2 (AAPL + Others)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Simulated accuracy scores based on previous confusion matrices\n",
    "accuracy_model1 = accuracy_score(y__test_1, y_prediction_1)\n",
    "accuracy_model2 = accuracy_score(y__test_2, y_prediction_2)\n",
    "\n",
    "# Data for the bar plot\n",
    "model_names = ['Model 1 (AAPL Only)', 'Model 2 (AAPL + Others)']\n",
    "accuracies = [accuracy_model1, accuracy_model2]\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(7, 5))\n",
    "bars = plt.bar(model_names, accuracies, color=['steelblue', 'seagreen'])\n",
    "\n",
    "# Add text labels with percentage above each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + 0.01, f\"{height:.2%}\", ha='center', fontsize=12)\n",
    "\n",
    "# Set chart details\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Comparison Between Model 1 and Model 2')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "# Compute raw (not normalized) confusion matrix for Model 1\n",
    "cm_model1 = confusion_matrix(y__test_1, y_prediction_1)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_model1, display_labels=[0, 1])\n",
    "disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('Model 1: Confusion Matrix (Raw Counts)')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Conclusion\n",
    "# print('The model correctly predicted 71 down days but wrongly predicted 102 of them as up.\\\n",
    "#       That''s about 41% accuracy for down days. For up days, the model correctly predicted 132 out of 178 → about 74% accuracy.\\\n",
    "#           Not bad! It still leans toward predicting price will go up, but less aggressively than Model 2.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute raw (not normalized) confusion matrix for Model 2\n",
    "cm_model2 = confusion_matrix(y__test_2, y_prediction_2)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_model2, display_labels=[0, 1])\n",
    "disp.plot(ax=ax, cmap='Greens', colorbar=False)\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('Model 2: Confusion Matrix (Raw Counts)')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Conclusion\n",
    "# print('When the stock actually went down, the model was wrong most of the time — \\\n",
    "#       it predicted \"up\" instead. Only 380 correct out of 1,692 → that''s about 22% accuracy for down days.\\\n",
    "#           When the stock actually went up, the model did pretty well. It got 1483 out of 1812 correct → about 82% accuracy for up days.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion of the confusion matrix comparison:\n",
    "\n",
    "Model 1 is also biased toward predicting price increases, but not as much as Model 2.\n",
    "\n",
    "It makes fewer mistakes than Model 2 when predicting down days.\n",
    "\n",
    "model 1 might be more balanced and safer if you're trying to avoid risky false buy signals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
